{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37364bitcodevirtual781ec49050ec4827828a5f1f26684830",
   "display_name": "Python 3.7.3 64-bit ('codevirtual')"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impotar la libreria de spark\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iniciar contexto de Spark \n",
    "spark = SparkSession.builder.appName(\"Analisis de Datos-Big data\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leer el dataset \n",
    "#Como nuestro dataset no tiene encabezado, establecemos header en false\n",
    "#Nuestro dataframe utiliza los encabezados por default que es _c0, _c01,....., _cn\n",
    "df = spark.read.load(\"colocar_ruta_del_dataset\",\n",
    "            format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establecemos el nombre de las columnas de nuestro dataset\n",
    "newNameColumns = [\"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\", \"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\", \"root_shell\",\"su_attempted\", \"num_root\", \"num_file_creations\",\"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\",\"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\", \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\", \"dst_host_srv_count\",\"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\", \"dst_host_srv_serror_rate\", \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\",\"label\"]\n",
    "df_rename = df.toDF(*newNameColumns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------------+-------+\n|           label|  count|\n+----------------+-------+\n|          smurf.|2807886|\n|        neptune.|1072017|\n|         normal.| 972781|\n|          satan.|  15892|\n|        ipsweep.|  12481|\n|      portsweep.|  10413|\n|           nmap.|   2316|\n|           back.|   2203|\n|    warezclient.|   1020|\n|       teardrop.|    979|\n|            pod.|    264|\n|   guess_passwd.|     53|\n|buffer_overflow.|     30|\n|           land.|     21|\n|    warezmaster.|     20|\n|           imap.|     12|\n|        rootkit.|     10|\n|     loadmodule.|      9|\n|      ftp_write.|      8|\n|       multihop.|      7|\n+----------------+-------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#Se van contar la cantidad de datos distintos de la ultima columna que referencia\n",
    "# a los diferentes tipos de ataques que se tiene\n",
    "df_rename.groupBy(\"label\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Como K-mean trabaja solamente con valores numericos eliminarios algunas columnas no numericas\n",
    "#Columna 1 -> Tipo de conexion\n",
    "#Columna 2 -> Tipo de servicio\n",
    "#Columna 3 -> Si el usuario inicio sesión o no\n",
    "df_only_numeric = df_rename.drop(\"protocol_type\", \"service\", \"flag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primeramente extraemos todas las columnas numericas excepto la columna label\n",
    "numeric_only_col = df_only_numeric.columns[:37]\n",
    "\n",
    "#Importamos VectorAsembler, es un transformador que combina una lista dada \n",
    "# de columnas en una sola columna vectorial\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "# Luego vectorizamos\n",
    "df_va = VectorAssembler(inputCols = numeric_only_col, outputCol = \"featureVector\")\n",
    "new_df = df_va.transform(df_only_numeric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos el k-means para empezar a armar nuestro modelo\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "#Sin la especificación del valor de k en la inicialización de la clase KMeans, K toma por default el valor 2\n",
    "kmeans = KMeans()\n",
    "#Establecemos la columna de predicción\n",
    "kmeans.setPredictionCol(\"cluster\")\n",
    "#Establecemos el feactureCol\n",
    "kmeans.setFeaturesCol(\"featureVector\")\n",
    "#Creamos el modelo\n",
    "model = kmeans.fit(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[array([4.83401949e+01, 1.83462155e+03, 8.26203190e+02, 5.71611720e-06,\n       6.48779303e-04, 7.96173468e-06, 1.24376586e-02, 3.20510858e-05,\n       1.43529049e-01, 8.08830584e-03, 6.81851124e-05, 3.67464677e-05,\n       1.29349608e-02, 1.18874823e-03, 7.43095237e-05, 1.02114351e-03,\n       0.00000000e+00, 4.08294086e-07, 8.35165553e-04, 3.34973508e+02,\n       2.95267146e+02, 1.77970317e-01, 1.78036989e-01, 5.76648988e-02,\n       5.77299094e-02, 7.89884132e-01, 2.11796106e-02, 2.82608101e-02,\n       2.32981078e+02, 1.89214283e+02, 7.53713390e-01, 3.07109788e-02,\n       6.05051931e-01, 6.46410789e-03, 1.78091184e-01, 1.77885898e-01,\n       5.79276115e-02]), array([1.0999000e+04, 0.0000000e+00, 1.3099374e+09, 0.0000000e+00,\n       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n       1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n       1.0000000e+00, 1.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n       2.5500000e+02, 1.0000000e+00, 0.0000000e+00, 6.5000000e-01,\n       1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n       1.0000000e+00])]\n"
     ]
    }
   ],
   "source": [
    "#Obtenemos los valores centros \n",
    "center = model.clusterCenters()\n",
    "print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Como tenemos 23 distintos tipos de conexiones, podemos concluir que con k = 2 no es preciso nuestro modelo\n",
    "# Por tanto no podemos precisar nuestras agrupaciones en nuestro datos\n",
    "#Veremos como se grupo nuestro datos, volvemos a tranformar \n",
    "transformed = model.transform(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+----------------+-------+\n|cluster|           label|  count|\n+-------+----------------+-------+\n|      0|          smurf.|2807886|\n|      0|        neptune.|1072017|\n|      0|         normal.| 972781|\n|      0|          satan.|  15892|\n|      0|        ipsweep.|  12481|\n|      0|      portsweep.|  10412|\n|      0|           nmap.|   2316|\n|      0|           back.|   2203|\n|      0|    warezclient.|   1020|\n|      0|       teardrop.|    979|\n|      0|            pod.|    264|\n|      0|   guess_passwd.|     53|\n|      0|buffer_overflow.|     30|\n|      0|           land.|     21|\n|      0|    warezmaster.|     20|\n|      0|           imap.|     12|\n|      0|        rootkit.|     10|\n|      0|     loadmodule.|      9|\n|      0|      ftp_write.|      8|\n|      0|       multihop.|      7|\n|      0|            phf.|      4|\n|      0|           perl.|      3|\n|      0|            spy.|      2|\n|      1|      portsweep.|      1|\n+-------+----------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "#Selecionamos la columna cluster y label\n",
    "transformed.select(\"cluster\",\"label\").groupBy(\"cluster\", \"label\").count().orderBy([\"cluster\",\"count\"], ascending=[1,0]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eligiendo el valor mas optimo o el mejor valor de k para nuestro conjunto de datos \n",
    "#Para cada cierto valor de K podriamos medir la calidad de la agrupación, es un buen parametro para saber que k \n",
    "# es el mas optimo, entonces para medir eso, una buena agrupación es cuandos los puntos estan mas cercano al centroide, que en este caso seria la media de las distancias o la media de las distancias al cuadrado\n",
    "# Se define una funcion para ese calculo.\n",
    "# Y K-means no ofrece un metodo computeCost para calcular esa media.\n",
    "import random\n",
    "def clusteringScore(data, k):\n",
    "    vector = VectorAssembler(inputCols = numeric_only_col, outputCol = \"featureVector\")\n",
    "    new_df = df_va.transform(data)\n",
    "    kmeans = KMeans()\n",
    "    kmeans.setSeed(random.randint(1,1000))\n",
    "    kmeans.setK(k)\n",
    "    kmeans.setPredictionCol(\"cluster\")\n",
    "    kmeans.setFeaturesCol(\"featureVector\")\n",
    "    model = kmeans.fit(new_df)\n",
    "    return model.computeCost(new_df) / data.count()\n",
    "\n",
    "#Comenzando por k igual 20 hasta 100 con paso de 20\n",
    "for k in range(20,120,20):\n",
    "    media = clusteringScore(df_only_numeric, k)\n",
    "    print(k, media)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#En base eso podemos ver que para ciertos valores de k en nuestro conjunto de datos tenemos que k=80 se tiene un agrupamiento suboptimo y ademas digamos que puede parar antes de llegar a su optimo local.\n",
    "#Pero es importante siempre va a depender de nuestro valores iniciales de los centroides, normalmente se eligen de forma aleatoria, hay dos tipos de variantes que se encargan de eso que es k-means++ y K-means||, por default la libreia de spark implementa el k-means||\n",
    "# Por tanto podemos ir ajustando algunos valores de nuestro modelo para tener un resultado mas optimo\n",
    "# Como el numeros de iteraciones, y la cantidad minima de movimiento del centroide\n",
    "# A valores menores el algortimo deja que los centroides se muevan por mucho mas tiempo\n",
    "# Pero en ese caso ajustamos el numero maximo de iteracciones, para que no haga demasiado calculos\n",
    "def clusteringScore(data, k):\n",
    "    vector = VectorAssembler(inputCols = numeric_only_col, outputCol = \"featureVector\")\n",
    "    new_df = df_va.transform(data)\n",
    "    kmeans = KMeans()\n",
    "    #Nuevo valores establecidos\n",
    "    kmeans.setMaxIter(40)\n",
    "    kmeans.setTol(1.0e-5)\n",
    "    kmeans.setSeed(random.randint(1,1000))\n",
    "    kmeans.setK(k)\n",
    "    kmeans.setPredictionCol(\"cluster\")\n",
    "    kmeans.setFeaturesCol(\"featureVector\")\n",
    "    model = kmeans.fit(new_df)\n",
    "    return model.computeCost(new_df) / data.count()\n",
    "\n",
    "#Comenzando por k igual 20 hasta 100 con paso de 20\n",
    "for k in range(20,120,20):\n",
    "    media = clusteringScore(df_only_numeric, k)\n",
    "    print(k, media)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Digamos que con estos ajuste se podria, queremos encontrar un punto a partir del cual el aumento de k no reduzca mucho esa media que obtenemos\n",
    "#Por tanto podemos observa mediante los resultados que a partir de k igual a 100 disminuye esa media, podriamos encontrar un valor de k mas optimo por encima de los 100"
   ]
  }
 ]
}