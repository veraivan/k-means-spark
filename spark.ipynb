{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37364bittest3ed4ef473dade42bf8c655c1bc6eeaa16",
   "display_name": "Python 3.7.3 64-bit ('test3')"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analisis de detección de anomalias en trafico de red.\n",
    "#Impotar la libreria de spark\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iniciar contexto de Spark \n",
    "spark = SparkSession.builder.appName(\"Analisis de Datos-Big data\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leer el dataset \n",
    "#Como nuestro dataset no tiene encabezado, establecemos header en false\n",
    "#Nuestro dataframe utiliza los encabezados por default que es _c0, _c01,....., _cn\n",
    "df = spark.read.load(\"add path of dataset\",\n",
    "            format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establecemos el nombre de las columnas de nuestro dataset\n",
    "newNameColumns = [\"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\", \"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\", \"root_shell\",\"su_attempted\", \"num_root\", \"num_file_creations\",\"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\",\"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\", \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\", \"dst_host_srv_count\",\"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\", \"dst_host_srv_serror_rate\", \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\",\"label\"]\n",
    "df_rename = df.toDF(*newNameColumns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------------+-------+\n|           label|  count|\n+----------------+-------+\n|          smurf.|2807886|\n|        neptune.|1072017|\n|         normal.| 972781|\n|          satan.|  15892|\n|        ipsweep.|  12481|\n|      portsweep.|  10413|\n|           nmap.|   2316|\n|           back.|   2203|\n|    warezclient.|   1020|\n|       teardrop.|    979|\n|            pod.|    264|\n|   guess_passwd.|     53|\n|buffer_overflow.|     30|\n|           land.|     21|\n|    warezmaster.|     20|\n|           imap.|     12|\n|        rootkit.|     10|\n|     loadmodule.|      9|\n|      ftp_write.|      8|\n|       multihop.|      7|\n+----------------+-------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#Se van contar la cantidad de datos distintos de la ultima columna que referencia\n",
    "# a los diferentes tipos de ataques que se tiene\n",
    "df_rename.groupBy(\"label\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Como K-mean trabaja solamente con valores numericos eliminarios algunas columnas no numericas\n",
    "#Columna 1 -> Tipo de conexion\n",
    "#Columna 2 -> Tipo de servicio\n",
    "#Columna 3 -> Si el usuario inicio sesión o no\n",
    "df_only_numeric = df_rename.drop(\"protocol_type\", \"service\", \"flag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primeramente extraemos todas las columnas numericas excepto la columna label\n",
    "numeric_only_col = df_only_numeric.columns[:37]\n",
    "\n",
    "#Importamos VectorAsembler, es un transformador que combina una lista dada \n",
    "# de columnas en una sola columna vectorial\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "# Luego vectorizamos\n",
    "df_va = VectorAssembler(inputCols = numeric_only_col, outputCol = \"featureVector\")\n",
    "new_df = df_va.transform(df_only_numeric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos el k-means para empezar a armar nuestro modelo\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "#Sin la especificación del valor de k en la inicialización de la clase KMeans, K toma por default el valor 2\n",
    "kmeans = KMeans()\n",
    "#Establecemos la columna de predicción\n",
    "kmeans.setPredictionCol(\"cluster\")\n",
    "#Establecemos el feactureCol\n",
    "kmeans.setFeaturesCol(\"featureVector\")\n",
    "#Creamos el modelo\n",
    "model = kmeans.fit(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[array([4.83401949e+01, 1.83462155e+03, 8.26203190e+02, 5.71611720e-06,\n       6.48779303e-04, 7.96173468e-06, 1.24376586e-02, 3.20510858e-05,\n       1.43529049e-01, 8.08830584e-03, 6.81851124e-05, 3.67464677e-05,\n       1.29349608e-02, 1.18874823e-03, 7.43095237e-05, 1.02114351e-03,\n       0.00000000e+00, 4.08294086e-07, 8.35165553e-04, 3.34973508e+02,\n       2.95267146e+02, 1.77970317e-01, 1.78036989e-01, 5.76648988e-02,\n       5.77299094e-02, 7.89884132e-01, 2.11796106e-02, 2.82608101e-02,\n       2.32981078e+02, 1.89214283e+02, 7.53713390e-01, 3.07109788e-02,\n       6.05051931e-01, 6.46410789e-03, 1.78091184e-01, 1.77885898e-01,\n       5.79276115e-02]), array([1.0999000e+04, 0.0000000e+00, 1.3099374e+09, 0.0000000e+00,\n       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n       1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n       1.0000000e+00, 1.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n       2.5500000e+02, 1.0000000e+00, 0.0000000e+00, 6.5000000e-01,\n       1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n       1.0000000e+00])]\n"
     ]
    }
   ],
   "source": [
    "#Obtenemos los valores centros \n",
    "center = model.clusterCenters()\n",
    "print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Como tenemos 23 distintos tipos de conexiones, podemos concluir que con k = 2 no es preciso nuestro modelo\n",
    "# Por tanto no podemos precisar nuestras agrupaciones en nuestro datos\n",
    "#Veremos como se agrupo nuestro datos, volvemos a tranformar \n",
    "transformed = model.transform(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+----------------+-------+\n|cluster|           label|  count|\n+-------+----------------+-------+\n|      0|          smurf.|2807886|\n|      0|        neptune.|1072017|\n|      0|         normal.| 972781|\n|      0|          satan.|  15892|\n|      0|        ipsweep.|  12481|\n|      0|      portsweep.|  10412|\n|      0|           nmap.|   2316|\n|      0|           back.|   2203|\n|      0|    warezclient.|   1020|\n|      0|       teardrop.|    979|\n|      0|            pod.|    264|\n|      0|   guess_passwd.|     53|\n|      0|buffer_overflow.|     30|\n|      0|           land.|     21|\n|      0|    warezmaster.|     20|\n|      0|           imap.|     12|\n|      0|        rootkit.|     10|\n|      0|     loadmodule.|      9|\n|      0|      ftp_write.|      8|\n|      0|       multihop.|      7|\n+-------+----------------+-------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#Selecionamos la columna cluster y label\n",
    "transformed.select(\"cluster\",\"label\").groupBy(\"cluster\", \"label\").count().orderBy([\"cluster\",\"count\"], ascending=[1,0]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "20 1.9975507385456697e-07\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-4f4eef978aea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#Comenzando por k igual 20 hasta 100 con paso de 20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mmedia\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclusteringScore0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_only_numeric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmedia\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-4f4eef978aea>\u001b[0m in \u001b[0;36mclusteringScore0\u001b[0;34m(data, k)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetPredictionCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cluster\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetFeaturesCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"featureVector\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mdf_tranform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bigdata/k-means-spark/test3/lib/python3.7/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/bigdata/k-means-spark/test3/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bigdata/k-means-spark/test3/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bigdata/k-means-spark/test3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/bigdata/k-means-spark/test3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bigdata/k-means-spark/test3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Eligiendo el valor mas optimo o el mejor valor de k para nuestro conjunto de datos \n",
    "#Para cada cierto valor de K podriamos medir la calidad de la agrupación, es un buen parametro para saber que k \n",
    "# es el mas optimo, entonces para medir eso, una buena agrupación es cuandos los puntos estan mas cercano al centroide, que en este caso seria la media de las distancias o la media de las distancias al cuadrado\n",
    "# Se define una funcion para ese calculo.\n",
    "# Y K-means no ofrece un metodo computeCost para calcular esa media.\n",
    "import random\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "def clusteringScore0(data, k):\n",
    "    vector = VectorAssembler(inputCols = numeric_only_col, outputCol = \"featureVector\")\n",
    "    new_df = vector.transform(data)\n",
    "    kmeans = KMeans()\n",
    "    kmeans.setSeed(random.randint(1,1000))\n",
    "    kmeans.setK(k)\n",
    "    kmeans.setPredictionCol(\"cluster\")\n",
    "    kmeans.setFeaturesCol(\"featureVector\")\n",
    "    model = kmeans.fit(new_df) \n",
    "\n",
    "    df_tranform = model.transform(new_df)\n",
    "    evaluator = ClusteringEvaluator()\n",
    "    evaluator.setFeaturesCol(\"featureVector\")\n",
    "    evaluator.setPredictionCol(\"cluster\")\n",
    "    return evaluator.evaluate(df_tranform) / df_tranform.count()\n",
    "\n",
    "#Comenzando por k igual 20 hasta 100 con paso de 20\n",
    "for k in range(20,120,20):\n",
    "    media = clusteringScore0(df_only_numeric, k)\n",
    "    print(k, media)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#En base eso podemos ver que para ciertos valores de k en nuestro conjunto de datos tenemos que k=80 se tiene un agrupamiento suboptimo y ademas digamos que puede parar antes de llegar a su optimo local.\n",
    "#Pero es importante siempre va a depender de nuestro valores iniciales de los centroides, normalmente se eligen de forma aleatoria, hay dos tipos de variantes que se encargan de eso que es k-means++ y K-means||, por default la libreia de spark implementa el k-means||\n",
    "# Por tanto podemos ir ajustando algunos valores de nuestro modelo para tener un resultado mas optimo\n",
    "# Como el numeros de iteraciones, y la cantidad minima de movimiento del centroide\n",
    "# A valores menores el algortimo deja que los centroides se muevan por mucho mas tiempo\n",
    "# Pero en ese caso ajustamos el numero maximo de iteracciones, para que no haga demasiado calculos\n",
    "def clusteringScore1(data, k):\n",
    "    vector = VectorAssembler(inputCols = numeric_only_col, outputCol = \"featureVector\")\n",
    "    new_df = vector.transform(data)\n",
    "    kmeans = KMeans()\n",
    "    #Nuevo valores establecidos\n",
    "    kmeans.setMaxIter(40)\n",
    "    kmeans.setTol(1.0e-5)\n",
    "    kmeans.setSeed(random.randint(1,1000))\n",
    "    kmeans.setK(k)\n",
    "    kmeans.setPredictionCol(\"cluster\")\n",
    "    kmeans.setFeaturesCol(\"featureVector\")\n",
    "    model = kmeans.fit(new_df)\n",
    "    \n",
    "    df_tranform = model.transform(new_df)\n",
    "    evaluator = ClusteringEvaluator()\n",
    "    evaluator.setFeaturesCol(\"featureVector\")\n",
    "    evaluator.setPredictionCol(\"cluster\")\n",
    "    return evaluator.evaluate(df_tranform) / df_tranform.count()\n",
    "\n",
    "#Comenzando por k igual 20 hasta 100 con paso de 20\n",
    "for k in range(20,120,20):\n",
    "    media = clusteringScore1(df_only_numeric, k)\n",
    "    print(k, media)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Digamos que con estos ajuste se podria encontrar un punto a partir del cual el aumento de k no reduzca mucho esa media que obtenemos.\n",
    "#Por tanto podemos observa mediante los resultados que a partir de k igual a 100 disminuye esa media, podriamos encontrar un valor de k mas optimo por encima de los 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pero si miramos graficamente, se observan que tenemos dos campos en donde su escala es mucho mayor que las demas,serian las columnas de bytes enviados y bytes recibidos, que eso obviamente va desde 0 a miles.\n",
    "#La idea tener una mejor escala se normalizan esos datos, de tal manera que todos nuestros datos esten normalizados para una mejor visualizacion\n",
    "#Se usa un metodo que nos ofrece MLib llamada StandardScaler para estandarizar esos features\n",
    "#Basicamente la normalización la resta de la media de los feature - su valor divido su desviación estandar \n",
    "#Se aumentara el valor de k para hacer pruebas con la normalización de nuestros datos \n",
    "\n",
    "#Se importa el metodo StandardScaler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "#Una version 2, con la normalizacion\n",
    "def clusteringScore2(data, k):\n",
    "    vector = VectorAssembler(inputCols = numeric_only_col, outputCol = \"featureVector\")\n",
    "    new_df = vector.transform(data)\n",
    "\n",
    "    #Normalización\n",
    "    standardScaler = StandardScaler()\n",
    "    standardScaler.setInputCol(\"featureVector\")\n",
    "    standardScaler.setOutputCol(\"scaledFeatureVector\")\n",
    "    #Ponemos em false, los datos centrales con media\n",
    "    standardScaler.setWithMean(False)\n",
    "    #Ponemos en verdadero la escala de la desviación stantard en una unidad\n",
    "    standardScaler.setWithStd(True)\n",
    "    fit_scaler = standardScaler.fit(new_df)\n",
    "    new_df = fit_scaler.transform(new_df)\n",
    "\n",
    "    kmeans = KMeans()\n",
    "    #Nuevo valores establecidos\n",
    "    kmeans.setMaxIter(40)\n",
    "    kmeans.setTol(1.0e-5)\n",
    "    kmeans.setSeed(random.randint(1,1000))\n",
    "    kmeans.setK(k)\n",
    "    kmeans.setPredictionCol(\"cluster\")\n",
    "    kmeans.setFeaturesCol(\"scaledFeatureVector\")\n",
    "    model = kmeans.fit(new_df)\n",
    "    \n",
    "    df_tranform = model.transform(new_df)\n",
    "    evaluator = ClusteringEvaluator()\n",
    "    evaluator.setFeaturesCol(\"scaledFeatureVector\")\n",
    "    evaluator.setPredictionCol(\"cluster\")\n",
    "    return evaluator.evaluate(df_tranform)\n",
    "\n",
    "#Comenzando por k igual 60 hasta 270 con paso de 30\n",
    "for k in range(60,300,30):\n",
    "    media = clusteringScore2(df_only_numeric, k)\n",
    "    print(k, media)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Si vemos los resultados, se puede observar que todavia no tenemos un valor de k apartir de cual su aumento no mejore el coste(distancia absoluta entre puntos) digamos\n",
    "\n",
    "#De igual manera igual se puede mejorar mas la agrupación, para los analisis anteriores se dejaron de lado digamos las columnas que no eran numericas. Por tanto para este analisis seria una información valiosa si le tomamos en cuenta, por tanto tendriamos una mejor clusters mejor informada.\n",
    "\n",
    "#Importamos StringIndexer ,OneHotEncoder, StandarScale \n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, StandardScaler\n",
    "\n",
    "def oneHotDataSet(inputCol, data):\n",
    "    stringIndexer = StringIndexer()\n",
    "    stringIndexer.setInputCol(inputCol)\n",
    "    stringIndexer.setOutputCol(inputCol + \"_indexed\")\n",
    "    fit_indexer = stringIndexer.fit(data)\n",
    "    new_df = fit_indexer.transform(data)\n",
    "    \n",
    "    #tcp, upd, icmp [1,2,3] -> [1,0,0] [0,1,0]\n",
    "    #Con esta funcion se mapean las variables categoricas y convierte a valores binarios\n",
    "    onehotencoder = OneHotEncoder()\n",
    "    onehotencoder.setInputCol(inputCol + \"_indexed\")\n",
    "    onehotencoder.setOutputCol(inputCol + \"_vec\")\n",
    "    fit_encoder = onehotencoder.fit(new_df)\n",
    "    new_df = fit_encoder.transform(new_df)\n",
    "\n",
    "    return new_df\n",
    "\n",
    "def fitModelFinal(data, k):\n",
    "    data = oneHotDataSet(\"protocol_type\", data)\n",
    "    data = oneHotDataSet(\"service\", data)\n",
    "    data = oneHotDataSet(\"flag\", data)\n",
    "\n",
    "    vector = VectorAssembler(inputCols \n",
    "    = [\"protocol_type_vec\", \"service_vec\", \"flag_vec\"], outputCol = \"featureVector\")\n",
    "    new_df = vector.transform(data)\n",
    "\n",
    "    #Normalización\n",
    "    standardScaler = StandardScaler()\n",
    "    standardScaler.setInputCol(\"featureVector\")\n",
    "    standardScaler.setOutputCol(\"scaledFeatureVector\")\n",
    "    #Ponemos em false, los datos centrales con media\n",
    "    standardScaler.setWithMean(False)\n",
    "    #Ponemos en verdadero la escala de la desviación stantard en una unidad\n",
    "    standardScaler.setWithStd(True)\n",
    "    fit_scaler = standardScaler.fit(new_df)\n",
    "    new_df = fit_scaler.transform(data)\n",
    "    \n",
    "    kmeans = KMeans()\n",
    "    kmeans.setMaxIter(40)\n",
    "    kmeans.setTol(1.0e-5)\n",
    "    kmeans.setSeed(random.randint(1,1000))\n",
    "    kmeans.setK(k)\n",
    "    kmeans.setPredictionCol(\"cluster\")\n",
    "    kmeans.setFeaturesCol(\"scaledFeatureVector\")\n",
    "    model = kmeans.fit(new_df)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#La funcion final para detectar anomalia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "#Y el paso final es la funcion de detención, con un umbral alto se podria que el trafico es anomalo \n",
    "def detectAnomaly():\n",
    "    modelo = fitModelFinal(df_rename,180)\n",
    "    centroides = modelo.clusterCenters()\n",
    "    df = modelo.transform(df_rename)\n",
    "    return df\n",
    "\n",
    "tes_df = detectAnomaly()\n",
    "umbral = df.select(\"cluster\", \"scaledFeatureVector\").withColumn(\"value\",Vectors.squared_distance(centroides[col(\"cluster\")], col(\"scaledFeatureVector\"))).orderBy(\"value\",ascending=False).select(\"value\").show(100)\n",
    "anomalias = df.filter(Vectors.squared_distance(centroides[df.cluster],df.scaledFeatureVector) >= unmbral)anomalias.first()"
   ]
  }
 ]
}